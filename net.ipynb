{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k0q29Z3ukKI",
    "outputId": "b0d01c57-bafc-4795-df40-0523fc4bc699"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting Bio\n",
      "  Downloading bio-1.3.8-py3-none-any.whl (269 kB)\n",
      "\u001B[?25l\r\u001B[K     |█▏                              | 10 kB 16.7 MB/s eta 0:00:01\r\u001B[K     |██▍                             | 20 kB 11.0 MB/s eta 0:00:01\r\u001B[K     |███▋                            | 30 kB 9.0 MB/s eta 0:00:01\r\u001B[K     |████▉                           | 40 kB 8.2 MB/s eta 0:00:01\r\u001B[K     |██████                          | 51 kB 4.8 MB/s eta 0:00:01\r\u001B[K     |███████▎                        | 61 kB 5.6 MB/s eta 0:00:01\r\u001B[K     |████████▌                       | 71 kB 5.5 MB/s eta 0:00:01\r\u001B[K     |█████████▊                      | 81 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |███████████                     | 92 kB 6.0 MB/s eta 0:00:01\r\u001B[K     |████████████▏                   | 102 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |█████████████▍                  | 112 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |██████████████▋                 | 122 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |███████████████▉                | 133 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████               | 143 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████▏             | 153 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████▍            | 163 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████▋           | 174 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████▉          | 184 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 194 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████▎       | 204 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▌      | 215 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▊     | 225 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████    | 235 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |█████████████████████████████▏  | 245 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████▍ | 256 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████▋| 266 kB 5.4 MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 269 kB 5.4 MB/s \n",
      "\u001B[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from Bio) (2.23.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from Bio) (4.64.0)\n",
      "Collecting mygene\n",
      "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
      "Collecting biopython>=1.79\n",
      "  Downloading biopython-1.79-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.3 MB)\n",
      "\u001B[K     |████████████████████████████████| 2.3 MB 43.3 MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from biopython>=1.79->Bio) (1.21.6)\n",
      "Collecting biothings-client>=0.2.6\n",
      "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->Bio) (2021.10.8)\n",
      "Installing collected packages: biothings-client, mygene, biopython, Bio\n",
      "Successfully installed Bio-1.3.8 biopython-1.79 biothings-client-0.2.6 mygene-3.2.2\n",
      "Collecting import-ipynb\n",
      "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.3.0)\n",
      "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import-ipynb) (5.5.0)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.7.5)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.4.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (57.4.0)\n",
      "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (1.0.18)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (2.6.1)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from IPython->import-ipynb) (0.8.1)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (1.15.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.10.0)\n",
      "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (2.15.3)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import-ipynb) (4.3.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (5.7.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.18.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (4.11.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import-ipynb) (3.8.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import-ipynb) (0.7.0)\n",
      "Installing collected packages: import-ipynb\n",
      "Successfully installed import-ipynb-0.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install Bio\n",
    "!pip install import-ipynb\n",
    "# !pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54RTxaK0YbHA",
    "outputId": "aa26bb0f-ad88-490d-e2ed-6f79576e27ad"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCeXnsvlLtE-",
    "outputId": "7c00a838-f51e-40a2-eb2f-c299c184b02e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/drive/.shortcut-targets-by-id/13JKG3wgYlOO5t5AoOpkix7QiZamDhSEp/ColabNotebooks\n",
      "importing Jupyter notebook from utils.ipynb\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "# import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# so we can import utils notebook (delete if working on Pycharm), you might need to change it to your working directory path\n",
    "%cd \"/content/drive/MyDrive/ColabNotebooks\"\n",
    "import import_ipynb\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6y4fRqWLLwhR"
   },
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "#                                                                             #\n",
    "#              Parameters you can change, but don't have to                   #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# number of ResNet blocks for the first ResNet and the kernel size.\n",
    "RESNET_1_BLOCKS = 2\n",
    "RESNET_1_KERNEL_SIZE = 3\n",
    "RESNET_1_KERNEL_NUM = 43\n",
    "\n",
    "###############################################################################\n",
    "#                                                                             #\n",
    "#                        Parameters you need to choose                        #\n",
    "#                                                                             #\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# number of ResNet blocks for the second ResNet, dilation list to repeat and the kernel size.\n",
    "\n",
    "RESNET_2_BLOCKS = 5\n",
    "RESNET_2_KERNEL_SIZE = 7  # good start may be 3/5\n",
    "RESNET_2_KERNEL_NUM = 53 # DO NOT MAKE IT 1!\n",
    "DILATION = [1]\n",
    "WANTED_M = len(DILATION)  # len of DILATION to be randomize by 'wandb' tool\n",
    "\n",
    "# percentage of dropout for the dropout layer\n",
    "DROPOUT = 0.29715545068   # good start may be 0.1-0.5\n",
    "\n",
    "# number of epochs, Learning rate and Batch size\n",
    "EPOCHS = 10\n",
    "LR = 0.0051378873577  # good start may be 0.0001/0.001/0.01\n",
    "BATCH = 128  # good start may be 32/64/128\n",
    "\n",
    "\n",
    "def get_time():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%d-%m-%Y__%H-%M-%S\")\n",
    "\n",
    "\n",
    "def resnet_block(input_layer, kernel_size, kernel_num, dialation=1):\n",
    "    bn1 = layers.BatchNormalization()(input_layer)\n",
    "    conv1d_layer1 = layers.Conv1D(kernel_num, kernel_size, padding='same', activation='relu',\n",
    "                                  dilation_rate=dialation)(bn1)\n",
    "    bn2 = layers.BatchNormalization()(conv1d_layer1)\n",
    "    conv1d_layer2 = layers.Conv1D(kernel_num, kernel_size, padding='same', activation='relu',\n",
    "                                  dilation_rate=dialation)(bn2)\n",
    "    return layers.Add()([input_layer, conv1d_layer2])\n",
    "\n",
    "\n",
    "def resnet_1(input_layer, block_num=RESNET_1_BLOCKS, kernel_size=RESNET_1_KERNEL_SIZE,\n",
    "             kernel_num=RESNET_1_KERNEL_NUM):\n",
    "    \"\"\"\n",
    "    ResNet layer - input -> BatchNormalization -> Conv1D -> Relu -> BatchNormalization -> Conv1D -> Relu -> Add\n",
    "    :param input_layer: input layer for the ResNet\n",
    "    :return: last layer of the ResNet\n",
    "    \"\"\"\n",
    "    last_layer_output = input_layer\n",
    "\n",
    "    for i in range(block_num):\n",
    "        last_layer_output = resnet_block(last_layer_output, kernel_size, kernel_num)\n",
    "\n",
    "    return last_layer_output\n",
    "\n",
    "\n",
    "def resnet_2(input_layer, block_num=RESNET_2_BLOCKS, kernel_size=RESNET_2_KERNEL_SIZE,\n",
    "             kernel_num=RESNET_2_KERNEL_NUM, dial_lst=DILATION):\n",
    "    \"\"\"\n",
    "    Dilated ResNet layer - input -> BatchNormalization -> dilated Conv1D -> Relu -> BatchNormalization -> dilated Conv1D -> Relu -> Add\n",
    "    :param input_layer: input layer for the ResNet\n",
    "    :return: last layer of the ResNet\n",
    "    \"\"\"\n",
    "    last_layer_output = input_layer\n",
    "\n",
    "    for i in range(block_num):\n",
    "        for d in dial_lst:\n",
    "            last_layer_output = resnet_block(last_layer_output, kernel_size, kernel_num, d)\n",
    "\n",
    "    return last_layer_output\n",
    "\n",
    "def get_default_config():\n",
    "  \"\"\"\n",
    "  :return: a configuration with the default \n",
    "  \"\"\"\n",
    "    sweep_config = {'RESNET_1_BLOCKS': RESNET_1_BLOCKS,\n",
    "                    'RESNET_1_KERNEL_SIZE': RESNET_1_KERNEL_SIZE,\n",
    "                    'RESNET_1_KERNEL_NUM': RESNET_1_KERNEL_NUM,\n",
    "                    'RESNET_2_BLOCKS': RESNET_2_BLOCKS,\n",
    "                    'RESNET_2_KERNEL_SIZE': RESNET_2_KERNEL_SIZE,\n",
    "                    'RESNET_2_KERNEL_NUM': RESNET_2_KERNEL_NUM,\n",
    "                    'DROPOUT': DROPOUT, 'EPOCHS': EPOCHS, \"LR\": LR,\n",
    "                    'DILATATION': DILATION, 'BATCH': BATCH, 'method': 'random',\n",
    "                    'metric': {'name': 'loss', 'goal': 'minimize'},\n",
    "                    'name': f\"BioEx4_{get_time()}\"}\n",
    "\n",
    "    return sweep_config\n",
    "\n",
    "def build_network(config=None):\n",
    "    \"\"\"\n",
    "    builds the neural network architecture as shown in the exercise.\n",
    "    :return: a Keras Model\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "\n",
    "    # input, shape (NB_MAX_LENGTH,FEATURE_NUM)\n",
    "    input_layer = tf.keras.Input(shape=(utils.NB_MAX_LENGTH, utils.FEATURE_NUM))\n",
    "\n",
    "    # Conv1D -> shape = (NB_MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
    "    conv1d_layer = layers.Conv1D(config['RESNET_1_KERNEL_NUM'], config['RESNET_1_KERNEL_SIZE'],\n",
    "                                 padding='same')(input_layer)\n",
    "\n",
    "    # first ResNet -> shape = (NB_MAX_LENGTH, RESNET_1_KERNEL_NUM)\n",
    "    resnet_layer = resnet_1(conv1d_layer, config['RESNET_1_BLOCKS'], config['RESNET_1_KERNEL_SIZE'],\n",
    "                            config['RESNET_1_KERNEL_NUM'])\n",
    "\n",
    "    # Conv1D -> shape = (NB_MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
    "    conv1d_layer = layers.Conv1D(config['RESNET_2_KERNEL_NUM'], config['RESNET_2_KERNEL_SIZE'],\n",
    "                                 padding=\"same\")(resnet_layer)\n",
    "\n",
    "    # second ResNet -> shape = (NB_MAX_LENGTH, RESNET_2_KERNEL_NUM)\n",
    "    resnet_layer = resnet_2(conv1d_layer, config['RESNET_2_BLOCKS'], config['RESNET_2_KERNEL_SIZE'],\n",
    "                            config['RESNET_2_KERNEL_NUM'], config['DILATATION'])\n",
    "\n",
    "    dp = layers.Dropout(config['DROPOUT'])(resnet_layer)\n",
    "    conv1d_layer = layers.Conv1D(config['RESNET_2_KERNEL_NUM'] // 2, config['RESNET_2_KERNEL_SIZE'],\n",
    "                                 padding=\"same\",\n",
    "                                 activation='elu')(dp)\n",
    "    dense = layers.Dense(15)(conv1d_layer)\n",
    "\n",
    "    return tf.keras.Model(input_layer, dense)\n",
    "\n",
    "\n",
    "def plot_val_train_loss(history):\n",
    "    \"\"\"\n",
    "    plots the train and validation loss of the model at each epoch, saves it in 'model_loss_history.png'\n",
    "    :param history: history object (output of fit function)\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    ig, axes = plt.subplots(1, 1, figsize=(15, 3))\n",
    "    axes.plot(history.history['loss'], label='Training loss')\n",
    "    axes.plot(history.history['val_loss'], label='Validation loss')\n",
    "    axes.legend()\n",
    "    axes.set_title(\"Train and Val MSE loss\")\n",
    "\n",
    "    plt.savefig(f\"/content/drive/MyDrive/ColabNotebooks/model_loss_history{get_time()}.png\")  # TODO: you can change the path here\n",
    "\n",
    "\n",
    "def get_config():\n",
    "    sweep_config = {}\n",
    "    sweep_config['method'] = 'bayes'\n",
    "    sweep_config['metric'] = {'name': 'best_val_loss', 'goal': 'minimize'}\n",
    "    sweep_config[\"early_terminate\"]= {\n",
    "        \"type\": \"hyperband\",\n",
    "        \"min_iter\": 2,\n",
    "        \"eta\": 2,\n",
    "    }\n",
    "\n",
    "    sweep_config['name'] = f\"BioEx4_{get_time()}\"\n",
    "    param_dict = {\n",
    "        'RESNET_1_BLOCKS': {'distribution': 'int_uniform', 'min': 1, 'max': 5},\n",
    "        'RESNET_1_KERNEL_SIZE': {'values': [3, 5, 7, 9]},\n",
    "        'RESNET_1_KERNEL_NUM': {'distribution': 'int_uniform', 'min': 8,\n",
    "                                'max': 64},\n",
    "        'RESNET_2_BLOCKS': {'distribution': 'int_uniform', 'min': 1, 'max': 5},\n",
    "        'RESNET_2_KERNEL_SIZE': {'values': [3, 5, 7, 9]},\n",
    "        'RESNET_2_KERNEL_NUM': {'distribution': 'int_uniform', 'min': 8,\n",
    "                                'max': 64},\n",
    "        'DROPOUT': {'distribution': 'uniform', 'min': 0.001, 'max': 0.5},\n",
    "        'EPOCHS': {'distribution': 'int_uniform', 'min': 5, 'max': 15},\n",
    "        \"LR\": {'distribution': 'uniform', 'min': 0.001, 'max': 0.025},\n",
    "        'BATCH': {'values': [16, 32, 64, 128, 256]},\n",
    "        'DILATATION': {'values': [[1, 2, 4], [1], [1, 2], [1, 4], [1, 2, 4, 8]]}\n",
    "    }\n",
    "\n",
    "    sweep_config['parameters'] = param_dict\n",
    "    return sweep_config\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class WandbCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, fold):\n",
    "        super(WandbCallback, self).__init__()\n",
    "        self.fold = fold\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        wandb.log({'loss': logs['loss'], 'val_loss': logs['val_loss'], 'fold':\n",
    "            self.fold, 'epoch': epoch})\n",
    "\n",
    "\n",
    "def models_selection(config=None):\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "    with wandb.init(config=config) as run:\n",
    "\n",
    "        # _______________loading the data_______________\n",
    "        config = wandb.config\n",
    "        input = np.load(\"train_input.npy\")  # numpy array of shape (1974,NB_MAX_LENGTH,FEATURE_NUM) - data\n",
    "        labels = np.load(\"train_labels.npy\")  # numpy array of shape (1974,NB_MAX_LENGTH,OUTPUT_SIZE) - labels\n",
    "        save_dir = \"BestFits/\"\n",
    "        model_name = run.name\n",
    "        fold_var = 1\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "        my_optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "        loss = 0\n",
    "        losses = np.zeros(5)\n",
    "        for t_idx, v_idx in kf.split(input, labels):\n",
    "            X_t, X_v = input[t_idx], input[v_idx]\n",
    "            y_t, y_v = labels[t_idx], labels[v_idx]\n",
    "\n",
    "            model = build_network(config)\n",
    "            # _______________compiling______________\n",
    "\n",
    "            model.compile(optimizer=my_optimizer, loss='mean_squared_error')\n",
    "\n",
    "            # _____________creating callbacks_____________\n",
    "            checkpoint = tf.keras.callbacks.ModelCheckpoint(f\"{save_dir}\"\n",
    "                                                            f\"{model_name}\"\n",
    "                                                            f\"{fold_var}.ckpt\",\n",
    "                                                            monitor='val_loss',\n",
    "                                                            save_best_only=True, mode='min')\n",
    "\n",
    "            callbacks_list = [checkpoint, WandbCallback(fold_var)]\n",
    "\n",
    "            # _____________fitting the model______________\n",
    "            history = model.fit(X_t, y_t,\n",
    "                                epochs=config['EPOCHS'],\n",
    "                                callbacks=callbacks_list,\n",
    "                                batch_size=config['BATCH'],\n",
    "                                validation_data=(X_v, y_v))\n",
    "\n",
    "\n",
    "            # _____________evaluate the model_____________\n",
    "            best_model = tf.keras.models.load_model(f\"{save_dir}\"\n",
    "                                                    f\"{model_name}\"\n",
    "                                                    f\"{fold_var}.ckpt\")\n",
    "\n",
    "            l = best_model.evaluate(X_v, y_v)\n",
    "            losses[fold_var - 1] = l\n",
    "            loss += l/5\n",
    "            wandb.log({'best_val_loss': loss})\n",
    "            # loss[fold_var - 1] = best_model.evaluate(X_v, y_v)\n",
    "            fold_var += 1\n",
    "            tf.keras.backend.clear_session()\n",
    "        wandb.log({'mean_loss': loss,'std':np.std(losses)})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train(config=None):\n",
    "    if config is None:\n",
    "        config = get_default_config()\n",
    "\n",
    "    # _______________loading the data_______________\n",
    "    input = np.load(\"train_input.npy\")  # numpy array of shape (1974,NB_MAX_LENGTH,FEATURE_NUM) - data\n",
    "    labels = np.load(\"train_labels.npy\")  # numpy array of shape (1974,NB_MAX_LENGTH,OUTPUT_SIZE) - labels\n",
    "    save_dir = \"BestFits/\"\n",
    "    model_name = \"selected_model\"\n",
    "    my_optimizer = tf.keras.optimizers.Adam(learning_rate=config['LR'])\n",
    "\n",
    "    model = build_network(config)\n",
    "    # _______________compiling______________\n",
    "\n",
    "    model.compile(optimizer=my_optimizer, loss='mean_squared_error')\n",
    "\n",
    "    # _____________fitting the model______________\n",
    "    history = model.fit(input, labels,\n",
    "                        epochs=config['EPOCHS'],\n",
    "                        callbacks=callbacks_list,\n",
    "                        batch_size=config['BATCH'],\n",
    "                        validation_split = 0.07)\n",
    "    plot_val_train_loss(history)\n",
    "    tf.keras.models.save_model(model, save_dir + model_name)\n",
    "    tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "id": "v8j1NEZC0uNF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qRN8JS77GEw2"
   },
   "outputs": [],
   "source": [
    "def part3():\n",
    "    model = tf.keras.models.load_model(save_dir + model_name)\n",
    "    input = utils.generate_input(\"6xw6/6xw6.pdb\")\n",
    "    prediction = model.predict(input[None,:,:])\n",
    "    seq=\"VQLQESGGGLVQAGDSLRVSCAASGRTISSSPMGWFRQAPGKEREFVAAISGNGGNTYYLDSVKGRFTTSRDNAKNTVYLQLNNLKPEDTAIYYCAARSRFSAMHLAYRRLVDYDDWGQGTQVTVS\"\n",
    "    utils.matrix_to_pdb(seq, prediction[0,:,:], \"prediction_6xw6_solar5\")\n",
    "\n",
    "def main():\n",
    "  # sweep_id = wandb.sweep(get_config(), project=\"BioEx4_5\",\n",
    "  #                          entity=\"avishai-elma\")\n",
    "  #   wandb.agent(sweep_id, models_selection, count=1000)\n",
    "  train()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "net.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}